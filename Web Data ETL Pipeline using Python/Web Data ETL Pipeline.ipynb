{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6afc525-4aeb-454b-8e2e-5ac204294ecc",
   "metadata": {},
   "source": [
    "A Web Data ETL (Extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources on the internet into a structured and usable format for analysis and storage\n",
    "\n",
    "Web Data ETL Pipeline: Process We Can Follow\n",
    "A Web Data ETL (Extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources on the internet into a structured and usable format for analysis and storage. It is essential for managing and processing large volumes of data gathered from websites, online platforms, and digital sources.\n",
    "\n",
    "The process begins with data extraction, where relevant information is collected from websites, APIs, databases, and other online sources.\n",
    "\n",
    "Then this raw data is transformed through various operations, including cleaning, filtering, structuring, and aggregating. After transformation, the data is loaded into a CSV file or database, making it accessible for further analysis, reporting, and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee8342-f5dd-43c3-8994-8db90935e353",
   "metadata": {},
   "source": [
    "let’s see how to build a Web Data ETL pipeline using Python. Here I will create a pipeline to scrape textual data from any article on the web. We will aim to store the data about the frequencies of each word in the article. I’ll start this task by importing the necessary Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96f7063-7b4b-490c-896e-f6ebb835305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Prince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use command for installing beautifulsoup and nltk: pip install beautifulsoup4 nltk\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184c1ef3-b6c0-4102-b984-1ab33ebe36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let’s start by extracting text from any article on the web\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        article_text = soup.get_text()\n",
    "        return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96e04c6-c5c2-4a5f-9898-a4b87886093a",
   "metadata": {},
   "source": [
    "In the above code, the WebScraper class provides a way to conveniently extract the main text content of an article from a given web page URL. By creating an instance of the WebScraper class and calling its extract_article_text method, we can retrieve the textual data of the article, which can then be further processed or analyzed as needed.\n",
    "\n",
    "As we want to store the frequency of each word in the article, we need to clean and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf17ab8-8ad1-4a9f-9bcb-e5e8e0de3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "\n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f16edb-40f6-45c6-8c2d-e5bf2db4f80d",
   "metadata": {},
   "source": [
    "In the above code, the TextProcessor class provides a convenient way to process text data by tokenizing it into words and cleaning those words by removing non-alphabetic words and stopwords. It is often a crucial step in text analysis and natural language processing tasks. By creating an instance of the TextProcessor class and calling its tokenize_and_clean method, you can obtain a list of cleaned and filtered words from a given input text.\n",
    "\n",
    "So till now, we have defined classes for scraping and preparing the data. Now we need to define a class for the entire ETL (Extract, Transform, Load) process for extracting article text, processing it, and generating a DataFrame of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4810c13-c4ce-4abb-a46a-14654c1d32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def run(self):\n",
    "        scraper = WebScraper(self.url)\n",
    "        article_text = scraper.extract_article_text()\n",
    "\n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "\n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=[\"Words\", \"Frequencies\"])\n",
    "        df = df.sort_values(by=\"Frequencies\", ascending=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316600d9-bdb5-46b7-9b0a-42af913433a2",
   "metadata": {},
   "source": [
    "In the above code, the ETLPipeline class encapsulates the end-to-end process of extracting article text from a web page, cleaning and processing the text, calculating word frequencies, and generating a sorted DataFrame. By creating an instance of the ETLPipeline class and calling its run method, you can perform the complete ETL process and obtain a DataFrame that provides insights into the most frequently used words in the article after removing stopwords.\n",
    "\n",
    "here’s how to run this pipeline to scrape textual data from any article from the web and count the frequency of each word in the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc4a58a-9831-4540-a614-5816b2db20dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Words  Frequencies\n",
      "3       data           24\n",
      "1     series           24\n",
      "0       time           21\n",
      "2   analysis           14\n",
      "12   science            8\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    article_url = \"https://amankharwal.medium.com/what-is-time-series-analysis-in-data-science-f89aaa1c0814\"\n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    print(result_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcca681-1c22-43f5-84a0-ba041822bf50",
   "metadata": {},
   "source": [
    "Summary\n",
    "A Web Data ETL (Extract, Transform, Load) pipeline is a systematic process used in data engineering to collect, transform, and load data from various sources on the internet into a structured and usable format for analysis and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a686cbdf-1eea-40ff-acc8-2995b14995a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
